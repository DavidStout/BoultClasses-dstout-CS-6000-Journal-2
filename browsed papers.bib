
@article{mcburney_towards_2018,
	title = {Towards {Prioritizing} {Documentation} {Effort}},
	issn = {0098-5589},
	doi = {10.1109/TSE.2017.2716950},
	abstract = {Programmers need documentation to comprehend software, but they often lack the time to write it. Thus, programmers must prioritize their documentation effort to ensure that sections of code important to program comprehension are thoroughly explained. In this paper, we explore the possibility of automatically prioritizing documentation effort. We performed two user studies to evaluate the effectiveness of static source code attributes and textual analysis of source code towards prioritizing documentation effort. The first study used open-source API Libraries while the second study was conducted using closed-source industrial software from ABB. Our findings suggest that static source code attributes are poor predictors of documentation effort priority, whereas textual analysis of source code consistently performed well as a predictor of documentation effort priority.},
	journal = {IEEE Transactions on Software Engineering},
	author = {McBurney, P. W. and Jiang, S. and Kessentini, M. and Kraft, N. A. and Armaly, A. and Mkaouer, M. W. and McMillan, C.},
	year = {2018},
	keywords = {code documentation, Documentation, Gold, Java, Libraries, Neural networks, program comprehension, Programming, Software, software maintenance},
	pages = {1--1},
	annote = {Browse time = 3mins 23 seconds,Trash; Not relevant to my current research and rather uninteresting results for automated documentation methods that are seemingly already done.},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Charles\\Zotero\\storage\\XMQVWZGZ\\7953505.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Charles\\Zotero\\storage\\QETM4S4C\\McBurney et al. - 2018 - Towards Prioritizing Documentation Effort.pdf:application/pdf}
}

@inproceedings{fu_easy_2017,
	address = {Paderborn, Germany},
	title = {Easy over hard: a case study on deep learning},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Easy over hard},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106256},
	doi = {10.1145/3106237.3106256},
	abstract = {While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a) a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b) other researchers to repeat, improve, or even refute that original work.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Fu, Wei and Menzies, Tim},
	year = {2017},
	pages = {49--60},
	annote = {Browse time: 2mins 12seconds, Trash; Paper deals with deep learning tuning and the results are needing to be updated at press time due to newer and faster deep learning methods.},
	file = {Fu and Menzies - 2017 - Easy over hard a case study on deep learning.pdf:C\:\\Users\\Charles\\Zotero\\storage\\BXAZQJE9\\Fu and Menzies - 2017 - Easy over hard a case study on deep learning.pdf:application/pdf}
}

@inproceedings{xie_loopster:_2017,
	address = {Paderborn, Germany},
	title = {Loopster: static loop termination analysis},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Loopster},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106260},
	doi = {10.1145/3106237.3106260},
	abstract = {Loop termination is an important problem for proving the correctness of a system and ensuring that the system always reacts. Existing loop termination analysis techniques mainly depend on the synthesis of ranking functions, which is often expensive. In this paper, we present a novel approach, named Loopster, which performs an efficient static analysis to decide the termination for loops based on path termination analysis and path dependency reasoning. Loopster adopts a divide-and-conquer approach: (1) we extract individual paths from a target multi-path loop and analyze the termination of each path, (2) analyze the dependencies between each two paths, and then (3) determine the overall termination of the target loop based on the relations among paths. We evaluate Loopster by applying it on the loop termination competition benchmark and three real-world projects. The results show that Loopster is effective in a majority of loops with better accuracy and 20×+ performance improvement compared to the state-of-the-art tools.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Xie, Xiaofei and Chen, Bihuan and Zou, Liang and Lin, Shang-Wei and Liu, Yang and Li, Xiaohong},
	year = {2017},
	pages = {84--94},
	annote = {Browse time: 3mins 5 seconds, Scan; Loop termination analysis techniques can be complicated and often expensive this paper proposes a more novel approach that I would like to read about further.},
	file = {Xie et al. - 2017 - Loopster static loop termination analysis.pdf:C\:\\Users\\Charles\\Zotero\\storage\\TVZPUHKF\\Xie et al. - 2017 - Loopster static loop termination analysis.pdf:application/pdf}
}

@inproceedings{yoga_fast_2017,
	address = {Paderborn, Germany},
	title = {A fast causal profiler for task parallel programs},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106254},
	doi = {10.1145/3106237.3106254},
	abstract = {This paper proposes TaskProf, a profiler that identifies parallelism bottlenecks in task parallel programs. It leverages the structure of a task parallel execution to perform fine-grained attribution of work to various parts of the program. TaskProf’s use of hardware performance counters to perform fine-grained measurements minimizes perturbation. TaskProf’s profile execution runs in parallel using multi-cores. TaskProf’s causal profile enables users to estimate improvements in parallelism when a region of code is optimized even when concrete optimizations are not yet known. We have used TaskProf to isolate parallelism bottlenecks in twenty three applications that use the Intel Threading Building Blocks library. We have designed parallelization techniques in five applications to increase parallelism by an order of magnitude using TaskProf. Our user study indicates that developers are able to isolate performance bottlenecks with ease using TaskProf.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Yoga, Adarsh and Nagarakatte, Santosh},
	year = {2017},
	pages = {15--26},
	annote = {Browse time: 2mins 28 seconds, Trash; Authors created a program that helps to identify parallel bottlenecks in code, while interesting it maybe relevant to current TA work with OS but not research.},
	file = {Yoga and Nagarakatte - 2017 - A fast causal profiler for task parallel programs.pdf:C\:\\Users\\Charles\\Zotero\\storage\\IXCDKHE9\\Yoga and Nagarakatte - 2017 - A fast causal profiler for task parallel programs.pdf:application/pdf}
}

@inproceedings{sidiroglou-douskos_codecarboncopy_2017,
	address = {Paderborn, Germany},
	title = {{CodeCarbonCopy}},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106269},
	doi = {10.1145/3106237.3106269},
	abstract = {We present CodeCarbonCopy (CCC), a system for transferring code from a donor application into a recipient application. CCC starts with functionality identified by the developer to transfer into an insertion point (again identified by the developer) in the recipient. CCC uses paired executions of the donor and recipient on the same input file to obtain a translation between the data representation and name space of the recipient and the data representation and name space of the donor. It also implements a static analysis that identifies and removes irrelevant functionality useful in the donor but not in the recipient. We evaluate CCC on eight transfers between six applications. Our results show that CCC can successfully transfer donor functionality into recipient applications.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Sidiroglou-Douskos, Stelios and Lahtinen, Eric and Eden, Anthony and Long, Fan and Rinard, Martin},
	year = {2017},
	pages = {95--105},
	annote = {Browse Time: 2 mins 55 seconds, Trash; Paper covers file transfers improvement between donor and recipient, not relevant or interesting to me.},
	file = {Sidiroglou-Douskos et al. - 2017 - CodeCarbonCopy.pdf:C\:\\Users\\Charles\\Zotero\\storage\\UPC64JVZ\\Sidiroglou-Douskos et al. - 2017 - CodeCarbonCopy.pdf:application/pdf}
}

@inproceedings{oh_finding_2017,
	address = {Paderborn, Germany},
	title = {Finding near-optimal configurations in product lines by random sampling},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106273},
	doi = {10.1145/3106237.3106273},
	abstract = {Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find nearoptimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Oh, Jeho and Batory, Don and Myers, Margaret and Siegmund, Norbert},
	year = {2017},
	pages = {61--71},
	annote = {Browse time: 2mins 6 seconds, Trash; Paper focuses on optimizing software product lines using random sampling techniques, outside of my field of interest.},
	file = {Oh et al. - 2017 - Finding near-optimal configurations in product lin.pdf:C\:\\Users\\Charles\\Zotero\\storage\\ET7FPQXX\\Oh et al. - 2017 - Finding near-optimal configurations in product lin.pdf:application/pdf}
}

@inproceedings{zhou_scalability_2017,
	address = {Paderborn, Germany},
	title = {On the scalability of {Linux} kernel maintainers' work},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106287},
	doi = {10.1145/3106237.3106287},
	abstract = {Open source software ecosystems evolve ways to balance the workload among groups of participants ranging from core groups to peripheral groups. As ecosystems grow, it is not clear whether the mechanisms that previously made them work will continue to be relevant or whether new mechanisms will need to evolve. The impact of failure for critical ecosystems such as Linux is enormous, yet the understanding of why they function and are effective is limited. We, therefore, aim to understand how the Linux kernel sustains its growth, how to characterize the workload of maintainers, and whether or not the existing mechanisms are scalable. We quantify maintainers’ work through the files that are maintained, and the change activity and the numbers of contributors in those files. We find systematic differences among modules; these differences are stable over time, which suggests that certain architectural features, commercial interests, or module-specific practices lead to distinct sustainable equilibria. We find that most of the modules have not grown appreciably over the last decade; most growth has been absorbed by a few modules. We also find that the effort per maintainer does not increase, even though the community has hypothesized that required effort might increase. However, the distribution of work among maintainers is highly unbalanced, suggesting that a few maintainers may experience increasing workload. We find that the practice of assigning multiple maintainers to a file yields only a power of 1/2 increase in productivity. We expect that our proposed framework to quantify maintainer practices will help clarify the factors that allow rapidly growing ecosystems to be sustainable.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Zhou, Minghui and Chen, Qingying and Mockus, Audris and Wu, Fengguang},
	year = {2017},
	pages = {27--37},
	annote = {Browse Time: 2 mins 51 seconds, Scan; Paper studies the impact of growth of software ecosystem of linux development and maintenance compared to relative workload/number of maintainers on each primary project, interesting to future linux kernel maintenance.},
	file = {Zhou et al. - 2017 - On the scalability of Linux kernel maintainers' wo.pdf:C\:\\Users\\Charles\\Zotero\\storage\\RHCV29T7\\Zhou et al. - 2017 - On the scalability of Linux kernel maintainers' wo.pdf:application/pdf}
}

@article{nongpoh_autosense:_2017,
	title = {{AutoSense}: {A} {Framework} for {Automated} {Sensitivity} {Analysis} of {Program} {Data}},
	volume = {43},
	issn = {0098-5589},
	shorttitle = {{AutoSense}},
	doi = {10.1109/TSE.2017.2654251},
	abstract = {In recent times, approximate computing is being increasingly adopted across the computing stack, from algorithms to computing hardware, to gain energy and performance efficiency by trading accuracy within acceptable limits. Approximation aware programming languages have been proposed where programmers can annotate data with type qualifiers (e.g., precise and approx) to denote its reliability. However, programmers need to judiciously annotate so that the accuracy loss remains within the desired limits. This can be non-trivial for large applications where error resilient and non-resilient program data may not be easily identifiable. Mis-annotation of even one data as error resilient/insensitive may result in an unacceptable output. In this paper, we present AutoSense, a framework to automatically classify resilient (insensitive) program data versus the sensitive ones with probabilistic reliability guarantee. AutoSense implements a combination of dynamic and static analysis methods for data sensitivity analysis. The dynamic analysis is based on statistical hypothesis testing, while the static analysis is based on classical data flow analysis. Experimental results compare our automated data classification with reported manual annotations on popular benchmarks used in approximate computing literature. AutoSense achieves promising reliability results compared to manual annotations and earlier methods, as evident from the experimental results.},
	number = {12},
	journal = {IEEE Transactions on Software Engineering},
	author = {Nongpoh, B. and Ray, R. and Dutta, S. and Banerjee, A.},
	month = dec,
	year = {2017},
	keywords = {accuracy loss, Approximate computing, approximate computing literature, approximation aware programming languages, automated data classification, automated sensitivity analysis, AutoSense, classical data flow analysis, computing hardware, computing stack, data flow analysis, data sensitivity analysis, dynamic analysis, energy efficiency, hypothesis testing, manual annotations, pattern classification, performance efficiency, Probabilistic logic, probabilistic reliability, probability, program diagnostics, program verification, Quality of service, resilient program data, sensitivity analysis, Sensitivity analysis, Sequential analysis, sequential probability ratio test, static analysis, statistical analysis, trading accuracy},
	pages = {1110--1124},
	annote = {Broswe Time: 3 mins 21 seconds, Trash; Authors create an automated framework to determine resilient datum in approximation aware programming languages, not in my current research interests.},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Charles\\Zotero\\storage\\APNDZCM8\\7820185.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Charles\\Zotero\\storage\\EYGDQVII\\Nongpoh et al. - 2017 - AutoSense A Framework for Automated Sensitivity A.pdf:application/pdf}
}

@article{wang_automatic_2018,
	title = {Automatic {Software} {Refactoring} via {Weighted} {Clustering} in {Method}-{Level} {Networks}},
	volume = {44},
	issn = {0098-5589},
	doi = {10.1109/TSE.2017.2679752},
	abstract = {In this study, we describe a system-level multiple refactoring algorithm, which can identify the move method, move field, and extract class refactoring opportunities automatically according to the principle of “high cohesion and low coupling.” The algorithm works by merging and splitting related classes to obtain the optimal functionality distribution from the system-level. Furthermore, we present a weighted clustering algorithm for regrouping the entities in a system based on merged method-level networks. Using a series of preprocessing steps and preconditions, the “bad smells” introduced by cohesion and coupling problems can be removed from both the non-inheritance and inheritance hierarchies without changing the code behaviors. We rank the refactoring suggestions based on the anticipated benefits that they bring to the system. Based on comparisons with related research and assessing the refactoring results using quality metrics and empirical evaluation, we show that the proposed approach performs well in different systems and is beneficial from the perspective of the original developers. Finally, an open source tool is implemented to support the proposed approach.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Wang, Y. and Yu, H. and Zhu, Z. and Zhang, W. and Zhao, Y.},
	month = mar,
	year = {2018},
	keywords = {automatic software refactoring, class refactoring opportunities, Clustering algorithms, Clustering analysis, cohesion, complex network, coupling, coupling problems, Couplings, high cohesion, inheritance hierarchies, low coupling, Measurement, merged method-level networks, merging classes, network theory (graphs), open source tool, optimal functionality distribution, Partitioning algorithms, pattern clustering, preprocessing steps, refactoring results, refactoring suggestions, Software algorithms, software maintenance, software quality, software refactoring, Software systems, splitting related classes, system-level multiple refactoring algorithm, weighted clustering algorithm},
	pages = {202--236},
	annote = {Browse time: 4 mins 21 seconds, Trash; Interesting concept and possibly useful to future research, but not in my current research interest.},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Charles\\Zotero\\storage\\9G7C7A9Z\\7874207.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Charles\\Zotero\\storage\\QENXA3PN\\Wang et al. - 2018 - Automatic Software Refactoring via Weighted Cluste.pdf:application/pdf}
}

@inproceedings{tsigkanos_modeling_2017,
	address = {Paderborn, Germany},
	title = {Modeling and verification of evolving cyber-physical spaces},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106299},
	doi = {10.1145/3106237.3106299},
	abstract = {We increasingly live in cyber-physical spaces – spaces that are both physical and digital, and where the two aspects are intertwined. Such spaces are highly dynamic and typically undergo continuous change. Software engineering can have a profound impact in this domain, by defining suitable modeling and specification notations as well as supporting design-time formal verification. In this paper, we present a methodology and a technical framework which support modeling of evolving cyber-physical spaces and reasoning about their spatio-temporal properties. We utilize a discrete, graph-based formalism for modeling cyber-physical spaces as well as primitives of change, giving rise to a reactive system consisting of rewriting rules with both local and global application conditions. Formal reasoning facilities are implemented adopting logic-based specification of properties and according model checking procedures, in both spatial and temporal fragments. We evaluate our approach using a case study of a disaster scenario in a smart city.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Tsigkanos, Christos and Kehrer, Timo and Ghezzi, Carlo},
	year = {2017},
	pages = {38--48},
	annote = {Browse time: 2 mins 7sec, Scan; Interesting concept and in a field that I find intriguing as well, the gap between cyber-physical space in narrowing and I think this paper may have further insight into potential future research.},
	file = {Tsigkanos et al. - 2017 - Modeling and verification of evolving cyber-physic.pdf:C\:\\Users\\Charles\\Zotero\\storage\\AMUFBYAT\\Tsigkanos et al. - 2017 - Modeling and verification of evolving cyber-physic.pdf:application/pdf}
}

@inproceedings{nelson_power_2017,
	address = {Paderborn, Germany},
	title = {The power of "why" and "why not": enriching scenario exploration with provenance},
	isbn = {978-1-4503-5105-8},
	shorttitle = {The power of "why" and "why not"},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106272},
	doi = {10.1145/3106237.3106272},
	abstract = {Scenario-finding tools like the Alloy Analyzer are widely used in numerous concrete domains like security, network analysis, UML analysis, and so on. They can help to verify properties and, more generally, aid in exploring a system’s behavior.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Nelson, Tim and Danas, Natasha and Dougherty, Daniel J. and Krishnamurthi, Shriram},
	year = {2017},
	pages = {106--116},
	annote = {Browse Time: 1 min 59 seconds, Trash; Interesting concept for it's field but answering porvenance scenario exploration using enhance scenario finders is not in my current area of research interest.},
	file = {Nelson et al. - 2017 - The power of why and why not enriching scenar.pdf:C\:\\Users\\Charles\\Zotero\\storage\\TVE3WPD9\\Nelson et al. - 2017 - The power of why and why not enriching scenar.pdf:application/pdf}
}

@inproceedings{fu_revisiting_2017,
	address = {Paderborn, Germany},
	title = {Revisiting unsupervised learning for defect prediction},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106257},
	doi = {10.1145/3106237.3106257},
	abstract = {Collecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore łunsupervisedž approaches to quality prediction that does not require labelled data. An alternate technique is to use łsupervisedž approaches that learn models from project data labelled with, say, łdefectivež or łnotdefectivež. Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects. At FSE’16, Yang et al. reported startling results where unsupervised defect predictors outperformed supervised predictors for efort-aware just-in-time defect prediction. If conirmed, these results would lead to a dramatic simpliication of a seemingly complex task (data mining) that is widely explored in the software engineering literature. This paper repeats and refutes those results as follows. (1) There is much variability in the eicacy of the Yang et al. predictors so even with their approach, some supervised data is required to prune weaker predictors away. (2) Their indings were grouped across N projects. When we repeat their analysis on a project-by-project basis, supervised predictors are seen to work better.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Fu, Wei and Menzies, Tim},
	year = {2017},
	pages = {72--83},
	annote = {Browse time: 2 mins 2sec, Trash; While this paper most definitely allows for future research, the field is not one that I am interested in currently but this paper maybe one to come back to.},
	file = {Fu and Menzies - 2017 - Revisiting unsupervised learning for defect predic.pdf:C\:\\Users\\Charles\\Zotero\\storage\\VBNHUSLA\\Fu and Menzies - 2017 - Revisiting unsupervised learning for defect predic.pdf:application/pdf}
}

@inproceedings{bohme_where_2017,
	address = {Paderborn, Germany},
	title = {Where is the bug and how is it fixed? an experiment with practitioners},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Where is the bug and how is it fixed?},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106255},
	doi = {10.1145/3106237.3106255},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Böhme, Marcel and Soremekun, Ezekiel O. and Chattopadhyay, Sudipta and Ugherughe, Emamurho and Zeller, Andreas},
	year = {2017},
	pages = {117--128},
	annote = {Browse Time: 2mins 8 sec, Trash; Definitely relevant to the field of software testing, this paper introduces a benchmarking utility for current automated bug/fault analysis methods, maybe useful in future research.},
	file = {Böhme et al. - 2017 - Where is the bug and how is it fixed an experimen.pdf:C\:\\Users\\Charles\\Zotero\\storage\\8AK8SIQ8\\Böhme et al. - 2017 - Where is the bug and how is it fixed an experimen.pdf:application/pdf}
}

@inproceedings{coelho_why_2017,
	address = {Paderborn, Germany},
	title = {Why modern open source projects fail},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106246},
	doi = {10.1145/3106237.3106246},
	abstract = {Open source is experiencing a renaissance period, due to the appearance of modern platforms and work�ows for developing and maintaining public code. As a result, developers are creating open source software at speeds never seen before. Consequently, these projects are also facing unprecedented mortality rates. To better understand the reasons for the failure of modern open source projects, this paper describes the results of a survey with the maintainers of 104 popular GitHub systems that have been deprecated. We provide a set of nine reasons for the failure of these open source projects. We also show that some maintenance practices—speci�cally the adoption of contributing guidelines and continuous integration—have an important association with a project failure or success. Finally, we discuss and reveal the principal strategies developers have tried to overcome the failure of the studied projects.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Coelho, Jailton and Valente, Marco Tulio},
	year = {2017},
	pages = {186--196},
	annote = {Browse Time: 2mins 3sec, Trash; Topic is something that seemingly doesn't need to be explored, the mortality rate of open sourced projects seems to correlated with the amount of open source projects.},
	file = {Coelho and Valente - 2017 - Why modern open source projects fail.pdf:C\:\\Users\\Charles\\Zotero\\storage\\HZGKAE75\\Coelho and Valente - 2017 - Why modern open source projects fail.pdf:application/pdf}
}

@inproceedings{murali_bayesian_2017,
	address = {Paderborn, Germany},
	title = {Bayesian specification learning for finding {API} usage errors},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106284},
	doi = {10.1145/3106237.3106284},
	abstract = {We present a Bayesian framework that can learn probabilistic specifications from large, unstructured code corpora, and then use these specifications to statically detect anomalous, hence likely buggy, program behavior. Our key insight is to build a statistical model that correlates specifications hidden inside a corpus with the syntax and observed behavior of their implementations. While analyzing a program, we condition this model into a posterior distribution that prioritizes specifications that are relevant to the program. The problem of finding anomalies is framed quantitatively, as a problem of computing a distance between a “reference distribution” over program behaviors that our model expects from the program, and the distribution over behaviors that the program actually produces. We implement our ideas in a system, called Salento, for finding API usage errors in Android programs. Salento learns specifications using a combination of a topic model and a neural network model. Our experiments show that the system can discover subtle errors in Android applications in the wild, and outperforms a comparable non-Bayesian approach.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Murali, Vijayaraghavan and Chaudhuri, Swarat and Jermaine, Chris},
	year = {2017},
	pages = {151--162},
	annote = {Browse Time: 2mins 30sec, Trash; Topic isn't clearly defined in the abstract without the proper prior knowledge of this area of study, not in my current interests.},
	file = {Murali et al. - 2017 - Bayesian specification learning for finding API us.pdf:C\:\\Users\\Charles\\Zotero\\storage\\RAILLGYZ\\Murali et al. - 2017 - Bayesian specification learning for finding API us.pdf:application/pdf}
}

@inproceedings{gopstein_understanding_2017,
	address = {Paderborn, Germany},
	title = {Understanding misunderstandings in source code},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106264},
	doi = {10.1145/3106237.3106264},
	abstract = {Humans often mistake the meaning of source code, and so misjudge a program’s true behavior. These mistakes can be caused by extremely small, isolated patterns in code, which can lead to signiicant runtime errors. These patterns are used in large, popular software projects and even recommended in style guides. To identify code patterns that may confuse programmers we extracted a preliminary set of ‘atoms of confusion’ from known confusing code. We show empirically in an experiment with 73 participants that these code patterns can lead to a signiicantly increased rate of misunderstanding versus equivalent code without the patterns. We then go on to take larger confusing programs and measure (in an experiment with 43 participants) the impact, in terms of programmer confusion, of removing these confusing patterns. All of our instruments, analysis code, and data are publicly available online for replication, experimentation, and feedback.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Gopstein, Dan and Iannacone, Jake and Yan, Yu and DeLong, Lois and Zhuang, Yanyan and Yeh, Martin K.-C. and Cappos, Justin},
	year = {2017},
	pages = {129--139},
	annote = {Browse Time: 1 min 57 sec, Scan; Authors are individuals that I currently work with and paper is directly in my field of interest/research.},
	file = {Gopstein et al. - 2017 - Understanding misunderstandings in source code.pdf:C\:\\Users\\Charles\\Zotero\\storage\\TAZKUC6C\\Gopstein et al. - 2017 - Understanding misunderstandings in source code.pdf:application/pdf}
}

@article{menzies_are_2017,
	title = {Are {Delayed} {Issues} {Harder} to {Resolve}? {Revisiting} {Cost}-to-{Fix} of {Defects} throughout the {Lifecycle}},
	volume = {22},
	issn = {1382-3256, 1573-7616},
	shorttitle = {Are {Delayed} {Issues} {Harder} to {Resolve}?},
	url = {http://arxiv.org/abs/1609.04886},
	doi = {10.1007/s10664-016-9469-x},
	abstract = {Many practitioners and academics believe in a delayed issue effect (DIE); i.e. the longer an issue lingers in the system, the more effort it requires to resolve. This belief is often used to justify major investments in new development processes that promise to retire more issues sooner. This paper tests for the delayed issue effect in 171 software projects conducted around the world in the period from 2006--2014. To the best of our knowledge, this is the largest study yet published on this effect. We found no evidence for the delayed issue effect; i.e. the effort to resolve issues in a later phase was not consistently or substantially greater than when issues were resolved soon after their introduction. This paper documents the above study and explores reasons for this mismatch between this common rule of thumb and empirical data. In summary, DIE is not some constant across all projects. Rather, DIE might be an historical relic that occurs intermittently only in certain kinds of projects. This is a significant result since it predicts that new development processes that promise to faster retire more issues will not have a guaranteed return on investment (depending on the context where applied), and that a long-held truth in software engineering should not be considered a global truism.},
	number = {4},
	urldate = {2018-09-07},
	journal = {Empirical Software Engineering},
	author = {Menzies, Tim and Nichols, William and Shull, Forrest and Layman, Lucas},
	month = aug,
	year = {2017},
	note = {arXiv: 1609.04886},
	keywords = {Computer Science - Software Engineering, D.2.8},
	pages = {1903--1935},
	annote = {Browse Time: 2mins 20 sec, Trash; Paper debunks an commonly excepted assumption that issues will take a larger resource pool to fix the longer they are let through the SDLC, not in my current research interests.},
	file = {arXiv\:1609.04886 PDF:C\:\\Users\\Charles\\Zotero\\storage\\ZTNCSBXV\\Menzies et al. - 2017 - Are Delayed Issues Harder to Resolve Revisiting C.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Charles\\Zotero\\storage\\G6V6TAHQ\\1609.html:text/html}
}

@inproceedings{siegmund_measuring_2017,
	address = {Paderborn, Germany},
	title = {Measuring neural efficiency of program comprehension},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106268},
	doi = {10.1145/3106237.3106268},
	abstract = {Most modern software programs cannot be understood in their entirety by a single programmer. Instead, programmers must rely on a set of cognitive processes that aid in seeking, filtering, and shaping relevant information for a given programming task. Several theories have been proposed to explain these processes, such as “beacons,” for locating relevant code, and “plans,” for encoding cognitive models. However, these theories are decades old and lack validation with modern cognitive-neuroscience methods. In this paper, we report on a study using functional magnetic resonance imaging (fMRI) with 11 participants who performed program comprehension tasks. We manipulated experimental conditions related to beacons and layout to isolate specific cognitive processes related to bottom-up comprehension and comprehension based on semantic cues. We found evidence of semantic chunking during bottom-up comprehension and lower activation of brain areas during comprehension based on semantic cues, confirming that beacons ease comprehension.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Siegmund, Janet and Peitek, Norman and Parnin, Chris and Apel, Sven and Hofmeister, Johannes and Kästner, Christian and Begel, Andrew and Bethmann, Anja and Brechmann, André},
	year = {2017},
	pages = {140--150},
	annote = {Browse Time: 2mins 8sec, Scan; Interesting concept and possible link to future research in cognitive code confusion.},
	file = {Siegmund et al. - 2017 - Measuring neural efficiency of program comprehensi.pdf:C\:\\Users\\Charles\\Zotero\\storage\\5JXFL3LQ\\Siegmund et al. - 2017 - Measuring neural efficiency of program comprehensi.pdf:application/pdf}
}

@inproceedings{verma_synergistic_2017,
	address = {Paderborn, Germany},
	title = {Synergistic debug-repair of heap manipulations},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106263},
	doi = {10.1145/3106237.3106263},
	abstract = {We present Wolverine, an integrated Debug-Repair environment for heap manipulating programs. Wolverine facilitates stepping through a concrete program execution, provides visualizations of the abstract program states (as box-and-arrow diagrams) and integrates a novel, proof-directed repair algorithm to synthesize repair patches. To provide a seamless environment, Wolverine supports “hot-patching" of the generated repair patches, enabling the programmer to continue the debug session without requiring an abort-compile-debug cycle. We also propose new debug-repair possibilities, specification refinement and specification slicing made possible by Wolverine. We evaluate our framework on 1600 buggy programs (generated using fault injection) on a variety of data-structures like singly, doubly and circular linked-lists, Binary Search Trees, AVL trees, Red-Black trees and Splay trees; Wolverine could repair all the buggy instances within reasonable time (less than 5 sec in most cases). We also evaluate Wolverine on 247 (buggy) student submissions; Wolverine could repair more than 80\% of programs where the student had made a reasonable attempt.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Verma, Sahil and Roy, Subhajit},
	year = {2017},
	pages = {163--173},
	annote = {Browse Time: 2mins 16sec, Scan; Paper proposes an interesting solution to integrating a debug-repair environment for heap manipulating programs, results look promising.},
	file = {Verma and Roy - 2017 - Synergistic debug-repair of heap manipulations.pdf:C\:\\Users\\Charles\\Zotero\\storage\\UIPELNSE\\Verma and Roy - 2017 - Synergistic debug-repair of heap manipulations.pdf:application/pdf}
}

@inproceedings{hilton_trade-offs_2017,
	address = {Paderborn, Germany},
	title = {Trade-offs in continuous integration: assurance, security, and flexibility},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Trade-offs in continuous integration},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106270},
	doi = {10.1145/3106237.3106270},
	abstract = {Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI being a widely used activity in software engineering, we do not know what motivates developers to use CI, and what barriers and unmet needs they face. Without such knowledge, developers make easily avoidable errors, tool builders invest in the wrong direction, and researchers miss opportunities for improving the practice of CI.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Hilton, Michael and Nelson, Nicholas and Tunnell, Timothy and Marinov, Darko and Dig, Danny},
	year = {2017},
	pages = {197--207},
	annote = {Browse time: 2mins 10sec, Trash; The paper focuses on the needs and barriers that affect the continuous iterative SDLCs (like Agile), relevant to the field but not interesting to my current research.},
	file = {Hilton et al. - 2017 - Trade-offs in continuous integration assurance, s.pdf:C\:\\Users\\Charles\\Zotero\\storage\\KPILQLVW\\Hilton et al. - 2017 - Trade-offs in continuous integration assurance, s.pdf:application/pdf}
}

@inproceedings{ferles_failure-directed_2017,
	address = {Paderborn, Germany},
	title = {Failure-directed program trimming},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106249},
	doi = {10.1145/3106237.3106249},
	abstract = {This paper describes a new program simplification technique called program trimming that aims to improve the scalability and precision of safety checking tools. Given a program P, program trimming generates a new program P′ such that P and P′ are equi-safe (i.e., P′ has a bug if and only if P has a bug), but P′ has fewer execution paths than P. Since many program analyzers are sensitive to the number of execution paths, program trimming has the potential to improve the effectiveness of safety checking tools.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Ferles, Kostas and Wüstholz, Valentin and Christakis, Maria and Dillig, Isil},
	year = {2017},
	pages = {174--185},
	annote = {Browse Time: 2mins 7sec, Trash; Paper describes a technique and tool developed to optimize code and reduce code paths, not in current area of interest.},
	file = {Ferles et al. - 2017 - Failure-directed program trimming.pdf:C\:\\Users\\Charles\\Zotero\\storage\\KKD28R2E\\Ferles et al. - 2017 - Failure-directed program trimming.pdf:application/pdf}
}

@inproceedings{jabbarvand_droid:_2017,
	address = {Paderborn, Germany},
	title = {µ{Droid}: an energy-aware mutation testing framework for {Android}},
	isbn = {978-1-4503-5105-8},
	shorttitle = {µ{Droid}},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106244},
	doi = {10.1145/3106237.3106244},
	abstract = {The rising popularity of mobile apps deployed on battery-constrained devices underlines the need for effectively evaluating their energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. As a result, for energy testing, developers are relying on tests intended for evaluating the functional correctness of apps. Such tests may not be adequate for revealing energy defects and inefficiencies in apps. This paper presents an energy-aware mutation testing framework, called µDroid, that can be used by developers to assess the adequacy of their test suite for revealing energy-related defects. µDroid implements fifty energy-aware mutation operators and relies on a novel, automatic oracle to determine if a mutant can be killed by a test. Our evaluation on real-world Android apps shows the ability of proposed mutation operators for evaluating the utility of tests in revealing energy defects. Moreover, our automated oracle can detect whether tests kill the energy mutants with an overall accuracy of 94\%, thereby making it possible to apply µDroid automatically.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Jabbarvand, Reyhaneh and Malek, Sam},
	year = {2017},
	pages = {208--219},
	annote = {Browse Time: 2mins 2sec, Trash; The paper proposes a utility that can be used to measure the effectiveness of tests that reveal energy-related defects in mobile apps, not my area of research but interesting.},
	file = {Jabbarvand and Malek - 2017 - µDroid an energy-aware mutation testing framework.pdf:C\:\\Users\\Charles\\Zotero\\storage\\AYKAQH3Q\\Jabbarvand and Malek - 2017 - µDroid an energy-aware mutation testing framework.pdf:application/pdf}
}

@inproceedings{nair_using_2017,
	address = {Paderborn, Germany},
	title = {Using bad learners to find good configurations},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106238},
	doi = {10.1145/3106237.3106238},
	abstract = {Finding the optimally performing configuration of a software system for a given setting is often challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, building an accurate performance model can be very expensive (and is often infeasible in practice). The central insight of this paper is that exact performance values (e.g., the response time of a software system) are not required to rank configurations and to identify the optimal one. As shown by our experiments, performance models that are cheap to learn but inaccurate (with respect to the difference between actual and predicted performance) can still be used rank configurations and hence find the optimal configuration. This novel rank-based approach allows us to significantly reduce the cost (in terms of number of measurements of sample configuration) as well as the time required to build performance models. We evaluate our approach with 21 scenarios based on 9 software systems and demonstrate that our approach is beneficial in 16 scenarios; for the remaining 5 scenarios, an accurate model can be built by using very few samples anyway, without the need for a rank-based approach.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Nair, Vivek and Menzies, Tim and Siegmund, Norbert and Apel, Sven},
	year = {2017},
	pages = {257--267},
	annote = {Browse time: 2mins 17sec, Trash; The paper is exploring lower learning performance models to identify optimal performance models, interesting but not in my area of study.},
	file = {Nair et al. - 2017 - Using bad learners to find good configurations.pdf:C\:\\Users\\Charles\\Zotero\\storage\\B9K3J65M\\Nair et al. - 2017 - Using bad learners to find good configurations.pdf:application/pdf}
}

@inproceedings{cai_adaptively_2017,
	address = {Paderborn, Germany},
	title = {Adaptively generating high quality fixes for atomicity violations},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106239},
	doi = {10.1145/3106237.3106239},
	abstract = {It is difficult to fix atomicity violations correctly. Existing gate lock algorithm (GLA) simply inserts gate locks to serialize executions, which may introduce performance bugs and deadlocks. Synthesized context-aware gate locks (by Grail) require complex source code synthesis. We propose Fixer to adaptively fix atomicity violations. It firstly analyses the lock acquisitions of an atomicity violation. Then it either adjusts the existing lock scope or inserts a gate lock. The former addresses cases where some locks are used but fail to provide atomic accesses. For the latter, it infers the visibility (being global or a field of a class/struct) of the gate lock such that the lock only protects related accesses. For both cases, Fixer further eliminates new lock orders to avoid introducing deadlocks. Of course, Fixer can produce both kinds of fixes on atomicity violations with locks. The experimental results on 15 previously used atomicity violations show that: Fixer correctly fixed all 15 atomicity violations without introducing deadlocks. However, GLA and Grail both introduced 5 deadlocks. HFix (that only targets on fixing certain types of atomicity violations) only fixed 2 atomicity violations and introduced 4 deadlocks. Fixer also provides an alternative way to insert gate locks (by inserting gate locks with proper visibility) considering fix acceptance.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Cai, Yan and Cao, Lingwei and Zhao, Jing},
	year = {2017},
	pages = {303--314},
	annote = {Browse Time: 2mins 7 sec, Trash; The paper deals with gate locks and atomicity in multithreading, this field is interesting but the paper seems to assert that its proposal is wildly better than current techniques, not my area of research.},
	file = {Cai et al. - 2017 - Adaptively generating high quality fixes for atomi.pdf:C\:\\Users\\Charles\\Zotero\\storage\\8IS6WMZ4\\Cai et al. - 2017 - Adaptively generating high quality fixes for atomi.pdf:application/pdf}
}

@inproceedings{sadeghi_patdroid:_2017,
	address = {Paderborn, Germany},
	title = {{PATDroid}: permission-aware {GUI} testing of {Android}},
	isbn = {978-1-4503-5105-8},
	shorttitle = {{PATDroid}},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106250},
	doi = {10.1145/3106237.3106250},
	abstract = {Recent introduction of a dynamic permission system in Android, allowing the users to grant and revoke permissions after the installation of an app, has made it harder to properly test apps. Since an app’s behavior may change depending on the granted permissions, it needs to be tested under a wide range of permission combinations. At the state-of-the-art, in the absence of any automated tool support, a developer needs to either manually determine the interaction of tests and app permissions, or exhaustively re-execute tests for all possible permission combinations, thereby increasing the time and resources required to test apps. This paper presents an automated approach, called PATDroid, for efficiently testing an Android app while taking the impact of permissions on its behavior into account. PATDroid performs a hybrid program analysis on both an app under test and its test suite to determine which tests should be executed on what permission combinations. Our experimental results show that PATDroid significantly reduces the testing effort, yet achieves comparable code coverage and fault detection capability as exhaustively testing an app under all permission combinations.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Sadeghi, Alireza and Jabbarvand, Reyhaneh and Malek, Sam},
	year = {2017},
	pages = {220--232},
	annote = {Browse Time: 1min 46sec, Scan; This paper proposes an automated testing approach for Android apps that take permissions into account when running test suites, I feel that there maybe some crossover that I can use from this paper.},
	file = {Sadeghi et al. - 2017 - PATDroid permission-aware GUI testing of Android.pdf:C\:\\Users\\Charles\\Zotero\\storage\\268L56JW\\Sadeghi et al. - 2017 - PATDroid permission-aware GUI testing of Android.pdf:application/pdf}
}

@inproceedings{siegmund_attributed_2017,
	address = {Paderborn, Germany},
	title = {Attributed variability models: outside the comfort zone},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Attributed variability models},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106251},
	doi = {10.1145/3106237.3106251},
	abstract = {Variability models are often enriched with attributes, such as performance, that encode the influence of features on the respective attribute. In spite of their importance, there are only few attributed variability models available that have attribute values obtained from empirical, real-world observations and that cover interactions between features. But, what does it mean for research and practice when staying in the comfort zone of developing algorithms and tools in a setting where artificial attribute values are used and where interactions are neglected? This is the central question that we want to answer here. To leave the comfort zone, we use a combination of kernel density estimation and a genetic algorithm to rescale a given (real-world) attribute-value profile to a given variability model. To demonstrate the influence and relevance of realistic attribute values and interactions, we present a replication of a widely recognized, third-party study, into which we introduce realistic attribute values and interactions. We found statistically significant differences between the original study and the replication. We infer lessons learned to conduct experiments that involve attributed variability models. We also provide the accompanying tool Thor for generating attribute values including interactions. Our solution is shown to be agnostic about the given input distribution and to scale to large variability models.},
	language = {en},
	urldate = {2018-09-07},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Siegmund, Norbert and Sobernig, Stefan and Apel, Sven},
	year = {2017},
	pages = {268--278},
	annote = {Browse Time: 2mins 10 sec, Trash; Paper discusses the impact on research when staying in the comfort zone of developing algorithms, not interesting to me.},
	file = {Siegmund et al. - 2017 - Attributed variability models outside the comfort.pdf:C\:\\Users\\Charles\\Zotero\\storage\\EFFTGEH9\\Siegmund et al. - 2017 - Attributed variability models outside the comfort.pdf:application/pdf}
}

@inproceedings{guo_atexrace:_2017,
	address = {Paderborn, Germany},
	title = {{AtexRace}: across thread and execution sampling for in-house race detection},
	isbn = {978-1-4503-5105-8},
	shorttitle = {{AtexRace}},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106242},
	doi = {10.1145/3106237.3106242},
	abstract = {Data race is a major source of concurrency bugs. Dynamic data race detection tools (e.g., FastTrack) monitor the executions of a program to report data races occurring in runtime. However, such tools incur significant overhead that slows down and perturbs executions. To address the issue, the state-of-the-art dynamic data race detection tools (e.g., LiteRace) apply sampling techniques to selectively monitor memory accesses. Although they reduce overhead, they also miss many data races as confirmed by existing studies. Thus, practitioners face a dilemma on whether to use FastTrack, which detects more data races but is much slower, or LiteRace, which is faster but detects less data races. In this paper, we propose a new sampling approach to address the major limitations of current sampling techniques, which ignore the facts that a data race involves two threads and a program under testing is repeatedly executed. We develop a tool called AtexRace to sample memory accesses across both threads and executions. By selectively monitoring the pairs of memory accesses that have not been frequently observed in current and previous executions, AtexRace detects as many data races as FastTrack at a cost as low as LiteRace. We have compared AtexRace against FastTrack and LiteRace on both Parsec benchmark suite and a large-scale real-world MySQL Server with 223 test cases. The experiments confirm that AtexRace can be a replacement of FastTrack and LiteRace.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Guo, Yu and Cai, Yan and Yang, Zijiang},
	year = {2017},
	pages = {315--325},
	annote = {Browse time: 2min 37sec, Trash; Authors have a good start on detecting data races on multithreaded programs, but not my area of interest.},
	file = {Guo et al. - 2017 - AtexRace across thread and execution sampling for.pdf:C\:\\Users\\Charles\\Zotero\\storage\\KYSTH5E8\\Guo et al. - 2017 - AtexRace across thread and execution sampling for.pdf:application/pdf}
}

@inproceedings{linares-vasquez_enabling_2017,
	address = {Paderborn, Germany},
	title = {Enabling mutation testing for {Android} apps},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106275},
	doi = {10.1145/3106237.3106275},
	abstract = {Mutation testing has been widely used to assess the fault-detection effectiveness of a test suite, as well as to guide test case generation or prioritization. Empirical studies have shown that, while mutants are generally representative of real faults, an effective application of mutation testing requires “traditional" operators designed for programming languages to be augmented with operators specific to an application domain and/or technology. This paper proposes MDroid+, a framework for effective mutation testing of Android apps. First, we systematically devise a taxonomy of 262 types of Android faults grouped in 14 categories by manually analyzing 2,023 software artifacts from different sources (e.g., bug reports, commits). Then, we identified a set of 38 mutation operators, and implemented an infrastructure to automatically seed mutations in Android apps with 35 of the identified operators. The taxonomy and the proposed operators have been evaluated in terms of stillborn/trivial mutants generated and their capacity to represent real faults in Android apps, as compared to other well know mutation tools.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Linares-Vásquez, Mario and Bavota, Gabriele and Tufano, Michele and Moran, Kevin and Di Penta, Massimiliano and Vendome, Christopher and Bernal-Cárdenas, Carlos and Poshyvanyk, Denys},
	year = {2017},
	pages = {233--244},
	annote = {Browse Time: 1min 58sec, Trash; Paper proposes a testing framework that will allow mutation testing of Android Apps, interesting but not my current area of research interest.},
	file = {Linares-Vásquez et al. - 2017 - Enabling mutation testing for Android apps.pdf:C\:\\Users\\Charles\\Zotero\\storage\\2I2QMVUR\\Linares-Vásquez et al. - 2017 - Enabling mutation testing for Android apps.pdf:application/pdf}
}

@inproceedings{gazzillo_kmax:_2017,
	address = {Paderborn, Germany},
	title = {Kmax: finding all configurations of {Kbuild} makefiles statically},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Kmax},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106283},
	doi = {10.1145/3106237.3106283},
	abstract = {Feature-oriented software design is a useful paradigm for building and reasoning about highly-configurable software. By making variability explicit, feature-oriented tools and languages make program analysis tasks easier, such as bug-finding, maintenance, and more. But critical software, such as Linux, coreboot, and BusyBox rely instead on brittle tools, such as Makefiles, to encode variability, impeding variability-aware tool development. Summarizing Makefile behavior for all configurations is difficult, because Makefiles have unusual semantics, and exhaustive enumeration of all configurations is intractable in practice. Existing approaches use ad-hoc heuristics, missing much of the encoded variability in Makefiles. We present Kmax, a new static analysis algorithm and tool for Kbuild Makefiles. It is a family-based variability analysis algorithm, where paths are Boolean expressions of configuration options, called reaching configurations, and its abstract state enumerates string values for all configurations. Kmax localizes configuration explosion to the statement level, making precise analysis tractable. The implementation analyzes Makefiles from the Kbuild build system used by several low-level systems projects. Evaluation of Kmax on the Linux and BusyBox build systems shows it to be accurate, precise, and fast. It is the first tool to collect all source files and their configurations from Linux. Compared to previous approaches, Kmax is far more accurate and precise, performs with little overhead, and scales better.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Gazzillo, Paul},
	year = {2017},
	pages = {279--290},
	annote = {Browse Time: 2min 55sec, Trash; Paper proposes a static analysis algorithm for analyzing Makefile configurations, maybe interesting for future research but not now.},
	file = {Gazzillo - 2017 - Kmax finding all configurations of Kbuild makefil.pdf:C\:\\Users\\Charles\\Zotero\\storage\\U7CVY3TN\\Gazzillo - 2017 - Kmax finding all configurations of Kbuild makefil.pdf:application/pdf}
}

@inproceedings{guo_symbolic_2017,
	address = {Paderborn, Germany},
	title = {Symbolic execution of programmable logic controller code},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106245},
	doi = {10.1145/3106237.3106245},
	abstract = {Programmable logic controllers (PLCs) are specialized computers for automating a wide range of cyber-physical systems. Since these systems are often safety-critical, software running on PLCs need to be free of programming errors. However, automated tools for testing PLC software are lacking despite the pervasive use of PLCs in industry. We propose a symbolic execution based method, named S PLC, for automatically testing PLC software written in programming languages speci ed in the IEC 61131-3 standard. S PLC takes the PLC source code as input and translates it into C before applying symbolic execution, to systematically generate test inputs that cover both paths in each periodic task and interleavings of these tasks. Toward this end, we propose a number of PLC-speci c reduction techniques for identifying and eliminating redundant interleavings. We have evaluated S PLC on a large set of benchmark programs with both single and multiple tasks. Our experiments show that S PLC can handle these programs e ciently, and for multi-task PLC programs, our new reduction techniques outperform the state-of-the-art partial order reduction technique by more than two orders of magnitude.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Guo, Shengjian and Wu, Meng and Wang, Chao},
	year = {2017},
	pages = {326--336},
	annote = {Browse Time: 2min 13sec, Trash; Paper proposes methodology to test Programmable Logic Controllers by transferring PLC source to symbolic C and running test suites on it, not interesting currently.},
	file = {Guo et al. - 2017 - Symbolic execution of programmable logic controlle.pdf:C\:\\Users\\Charles\\Zotero\\storage\\FIXEEUB5\\Guo et al. - 2017 - Symbolic execution of programmable logic controlle.pdf:application/pdf}
}

@inproceedings{su_guided_2017,
	address = {Paderborn, Germany},
	title = {Guided, stochastic model-based {GUI} testing of {Android} apps},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106298},
	doi = {10.1145/3106237.3106298},
	abstract = {Mobile apps are ubiquitous, operate in complex environments and are developed under the time-to-market pressure. Ensuring their correctness and reliability thus becomes an important challenge. This paper introduces Stoat, a novel guided approach to perform stochastic model-based testing on Android apps. Stoat operates in two phases: (1) Given an app as input, it uses dynamic analysis enhanced by a weighted UI exploration strategy and static analysis to reverse engineer a stochastic model of the app’s GUI interactions; and (2) it adapts Gibbs sampling to iteratively mutate/refine the stochastic model and guides test generation from the mutated models toward achieving high code and model coverage and exhibiting diverse sequences. During testing, system-level events are randomly injected to further enhance the testing effectiveness. Stoat was evaluated on 93 open-source apps. The results show (1) the models produced by Stoat cover 17∼31\% more code than those by existing modeling tools; (2) Stoat detects 3X more unique crashes than two state-of-the-art testing tools, Monkey and Sapienz. Furthermore, Stoat tested 1661 most popular Google Play apps, and detected 2110 previously unknown and unique crashes. So far, 43 developers have responded that they are investigating our reports. 20 of reported crashes have been confirmed, and 8 already fixed.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong},
	year = {2017},
	pages = {245--256},
	annote = {Browse Time: 2min 53sec, Scan; Paper introduces STOAT, a stochastic based model for testing Android apps with promising results, interesting for future research.},
	file = {Su et al. - 2017 - Guided, stochastic model-based GUI testing of Andr.pdf:C\:\\Users\\Charles\\Zotero\\storage\\YCKH3BMN\\Su et al. - 2017 - Guided, stochastic model-based GUI testing of Andr.pdf:application/pdf}
}

@inproceedings{knuppel_is_2017,
	address = {Paderborn, Germany},
	title = {Is there a mismatch between real-world feature models and product-line research?},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106252},
	doi = {10.1145/3106237.3106252},
	abstract = {Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Knüppel, Alexander and Thüm, Thomas and Mennicke, Stephan and Meinicke, Jens and Schaefer, Ina},
	year = {2017},
	pages = {291--302},
	annote = {Browse Time: 2min 27sec, Trash; Paper abstract doesn't really make it clear what the authors are trying to accomplish, not interesting.},
	file = {Knüppel et al. - 2017 - Is there a mismatch between real-world feature mod.pdf:C\:\\Users\\Charles\\Zotero\\storage\\TZ224JLA\\Knüppel et al. - 2017 - Is there a mismatch between real-world feature mod.pdf:application/pdf}
}

@inproceedings{kusano_thread-modular_2017,
	address = {Paderborn, Germany},
	title = {Thread-modular static analysis for relaxed memory models},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106243},
	doi = {10.1145/3106237.3106243},
	abstract = {We propose a memory-model-aware static program analysis method for accurately analyzing the behavior of concurrent software running on processors with weak consistency models such as x86-TSO, SPARC-PSO, and SPARC-RMO. At the center of our method is a uni ed framework for deciding the feasibility of inter-thread interferences to avoid propagating spurious data ows during static analysis and thus boost the performance of the static analyzer. We formulate the checking of interference feasibility as a set of Datalog rules which are both e ciently solvable and general enough to capture a range of hardware-level memory models. Compared to existing techniques, our method can signi cantly reduce the number of bogus alarms as well as unsound proofs. We implemented the method and evaluated it on a large set of multithreaded C programs. Our experiments show the method signi cantly outperforms state-of-the-art techniques in terms of accuracy with only moderate runtime overhead.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Kusano, Markus and Wang, Chao},
	year = {2017},
	pages = {337--348},
	annote = {Browse Time: 2min 32sec, Trash; Paper introduces a method for a unified framework to sniff out potential inter-thread interferences on relaxed memory systems, not interesting to my current research.},
	file = {Kusano and Wang - 2017 - Thread-modular static analysis for relaxed memory .pdf:C\:\\Users\\Charles\\Zotero\\storage\\VQMGAI3J\\Kusano and Wang - 2017 - Thread-modular static analysis for relaxed memory .pdf:application/pdf}
}

@inproceedings{aliabadi_artinali:_2017,
	address = {Paderborn, Germany},
	title = {{ARTINALI}: dynamic invariant detection for cyber-physical system security},
	isbn = {978-1-4503-5105-8},
	shorttitle = {{ARTINALI}},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106282},
	doi = {10.1145/3106237.3106282},
	abstract = {Cyber-Physical Systems (CPSes) are being widely deployed in security-critical scenarios such as smart homes and medical devices. Unfortunately, the connectedness of these systems and their relative lack of security measures makes them ripe targets for attacks. Specification-based Intrusion Detection Systems (IDS) have been shown to be effective for securing CPSs. Unfortunately, deriving invariants for capturing the specifications of CPS systems is a tedious and error-prone process. Therefore, it is important to dynamically monitor the CPS system to learn its common behaviors and formulate invariants for detecting security attacks. Existing techniques for invariant mining only incorporate data and events, but not time. However, time is central to most CPS systems, and hence incorporating time in addition to data and events, is essential for achieving low false positives and false negatives. This paper proposes ARTINALI, which mines dynamic system properties by incorporating time as a first-class property of the system. We build ARTINALI-based Intrusion Detection Systems (IDSes) for two CPSes, namely smart meters and smart medical devices, and measure their efficacy. We find that the ARTINALIbased IDSes significantly reduce the ratio of false positives and false negatives by 16 to 48\% (average 30.75\%) and 89 to 95\% (average 93.4\%) respectively over other dynamic invariant detection tools.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Aliabadi, Maryam Raiyat and Kamath, Amita Ajith and Gascon-Samson, Julien and Pattabiraman, Karthik},
	year = {2017},
	pages = {349--361},
	annote = {Browse Time: 2min 8 sec, Scan; Paper presents an invariant detection scheme for use on Cyber-Physical systems, interesting and want to know why they only tested on two systems.
Scan notes:},
	file = {Aliabadi et al. - 2017 - ARTINALI dynamic invariant detection for cyber-ph.pdf:C\:\\Users\\Charles\\Zotero\\storage\\FSBZFFFP\\Aliabadi et al. - 2017 - ARTINALI dynamic invariant detection for cyber-ph.pdf:application/pdf}
}

@inproceedings{abdalkareem_why_2017,
	address = {Paderborn, Germany},
	title = {Why do developers use trivial packages? an empirical case study on npm},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Why do developers use trivial packages?},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106267},
	doi = {10.1145/3106237.3106267},
	abstract = {Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call ‘trivial packages’. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Abdalkareem, Rabe and Nourry, Olivier and Wehaibi, Sultan and Mujahid, Suhaib and Shihab, Emad},
	year = {2017},
	pages = {385--395},
	annote = {Browse Time: 2min 7sec, Trash; Empirical study of re-using code in trivial packages seems to be locked into the idea of ease of use, not interesting to current research.},
	file = {Abdalkareem et al. - 2017 - Why do developers use trivial packages an empiric.pdf:C\:\\Users\\Charles\\Zotero\\storage\\BTE2RNFM\\Abdalkareem et al. - 2017 - Why do developers use trivial packages an empiric.pdf:application/pdf}
}

@inproceedings{zibaeenejad_continuous_2017,
	address = {Paderborn, Germany},
	title = {Continuous variable-specific resolutions of feature interactions},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106302},
	doi = {10.1145/3106237.3106302},
	abstract = {Systems that are assembled from independently developed features suffer from feature interactions, in which features affect one another’s behaviour in surprising ways. The Feature Interaction Problem results from trying to implement an appropriate resolution for each interaction within each possible context, because the number of possible contexts to consider increases exponentially with the number of features in the system. Resolution strategies aim to combat the Feature Interaction Problem by offering default strategies that resolve entire classes of interactions, thereby reducing the work needed to resolve lots of interactions. However most such approaches employ coarse-grained resolution strategies (e.g., feature priority) or a centralized arbitrator.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Zibaeenejad, M. Hadi and Zhang, Chi and Atlee, Joanne M.},
	year = {2017},
	pages = {408--418},
	annote = {Browse Time: 2min 43sec, Trash; Paper discusses how features that are independently developed and combined in a system suffer from feature interactions and how to go about fixing them at runtime variable specific default resolution strategies, not interesting to current research.},
	file = {Zibaeenejad et al. - 2017 - Continuous variable-specific resolutions of featur.pdf:C\:\\Users\\Charles\\Zotero\\storage\\KB87XUMS\\Zibaeenejad et al. - 2017 - Continuous variable-specific resolutions of featur.pdf:application/pdf}
}

@inproceedings{kuvent_symbolic_2017,
	address = {Paderborn, Germany},
	title = {A symbolic justice violations transition system for unrealizable {GR}(1) specifications},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106240},
	doi = {10.1145/3106237.3106240},
	abstract = {One of the main challenges of reactive synthesis, an automated procedure to obtain a correct-by-construction reactive system, is to deal with unrealizable specifications. Existing approaches to deal with unrealizability, in the context of GR(1), an expressive assumeguarantee fragment of LTL that enables efficient synthesis, include the generation of concrete counter-strategies and the computation of an unrealizable core. Although correct, such approaches produce large and complicated counter-strategies, often containing thousands of states. This hinders their use by engineers. In this work we present the Justice Violations Transition System (JVTS), a novel symbolic representation of counter-strategies for GR(1). The JVTS is much smaller and simpler than its corresponding concrete counter-strategy. Moreover, it is annotated with invariants that explain how the counter-strategy forces the system to violate the specification. We compute the JVTS symbolically, and thus more efficiently, without the expensive enumeration of concrete states. Finally, we provide the JVTS with an on-demand interactive concrete and symbolic play.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Kuvent, Aviv and Maoz, Shahar and Ringert, Jan Oliver},
	year = {2017},
	pages = {362--372},
	annote = {Browse time: 2min 16sec, Trash; Paper deals with a novel symbolic representation system of counter-strategies for GR(1) a fragment of LTL, not clear in purpose or what its really doing from just the abstract and was therefore not interesting.},
	file = {Kuvent et al. - 2017 - A symbolic justice violations transition system fo.pdf:C\:\\Users\\Charles\\Zotero\\storage\\VZPGECRC\\Kuvent et al. - 2017 - A symbolic justice violations transition system fo.pdf:application/pdf}
}

@article{fleck_model_2017,
	title = {Model {Transformation} {Modularization} as a {Many}-{Objective} {Optimization} {Problem}},
	volume = {43},
	issn = {0098-5589},
	doi = {10.1109/TSE.2017.2654255},
	abstract = {Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.},
	number = {11},
	journal = {IEEE Transactions on Software Engineering},
	author = {Fleck, M. and Troya, J. and Kessentini, M. and Wimmer, M. and Alkhazi, B.},
	month = nov,
	year = {2017},
	keywords = {Adaptation models, Algorithm design and analysis, ATL, ATL transformations, automated search-based approach, bug fixing, Computer bugs, concrete transformation language, genetic algorithms, higher-order transformations, in-place transformation engine, maintenance activities, many-objective optimization problem, MDE, Measurement, metamodels version, Model transformation, model transformation modularization, model transformation programs, model transformations modularization, modularization, monolithic artifacts, NSGA-III, Object oriented modeling, program debugging, SBSE, search problems, Software engineering, software maintenance, software quality, statistical analysis, transformation languages, transformation rules, Unified modeling language},
	pages = {1009--1032},
	annote = {Browse Time: 3min 29sec, Trash; Paper subject deals with automating modularization in model transformations, I have no interest in this subject.},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Charles\\Zotero\\storage\\BZLEHNMW\\7820199.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Charles\\Zotero\\storage\\3A2KMWDX\\Fleck et al. - 2017 - Model Transformation Modularization as a Many-Obje.pdf:application/pdf}
}

@article{shevtsov_control-theoretical_2018,
	title = {Control-{Theoretical} {Software} {Adaptation}: {A} {Systematic} {Literature} {Review}},
	volume = {44},
	issn = {0098-5589},
	shorttitle = {Control-{Theoretical} {Software} {Adaptation}},
	doi = {10.1109/TSE.2017.2704579},
	abstract = {Modern software applications are subject to uncertain operating conditions, such as dynamics in the availability of services and variations of system goals. Consequently, runtime changes cannot be ignored, but often cannot be predicted at design time. Control theory has been identified as a principled way of addressing runtime changes and it has been applied successfully to modify the structure and behavior of software applications. Most of the times, however, the adaptation targeted the resources that the software has available for execution (CPU, storage, etc.) more than the software application itself. This paper investigates the research efforts that have been conducted to make software adaptable by modifying the software rather than the resource allocated to its execution. This paper aims to identify: the focus of research on control-theoretical software adaptation; how software is modeled and what control mechanisms are used to adapt software; what software qualities and controller guarantees are considered. To that end, we performed a systematic literature review in which we extracted data from 42 primary studies selected from 1,512 papers that resulted from an automatic search. The results of our investigation show that even though the behavior of software is considered non-linear, research efforts use linear models to represent it, with some success. Also, the control strategies that are most often considered are classic control, mostly in the form of Proportional and Integral controllers, and Model Predictive Control. The paper also discusses sensing and actuating strategies that are prominent for software adaptation and the (often neglected) proof of formal properties. Finally, we distill open challenges for control-theoretical software adaptation.},
	number = {8},
	journal = {IEEE Transactions on Software Engineering},
	author = {Shevtsov, S. and Berekmeri, M. and Weyns, D. and Maggio, M.},
	month = aug,
	year = {2018},
	keywords = {Adaptation models, Bibliographies, control theory, Control theory, control-theoretical software adaptation, Knowledge based systems, Mathematical model, model predictive control, modern software applications, predictive control, resource allocation, reverse engineering, Runtime, runtime changes, Self-adaptive software, Software, software adaptable, software adaptation, software application, software engineering, software performance evaluation, software qualities, software quality, systematic literature review},
	pages = {784--810},
	annote = {Browse Time: 2min 20sec, Trash; Paper deals with the use of control theory on modern software applications at runtime, not my area of current research interest.},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Charles\\Zotero\\storage\\EKTL2BIE\\7929422.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Charles\\Zotero\\storage\\R95GQ4C3\\Shevtsov et al. - 2018 - Control-Theoretical Software Adaptation A Systema.pdf:application/pdf}
}

@inproceedings{chaparro_detecting_2017,
	address = {Paderborn, Germany},
	title = {Detecting missing information in bug descriptions},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106285},
	doi = {10.1145/3106237.3106285},
	abstract = {Bug reports document unexpected software behaviors experienced by users. To be effective, they should allow bug triagers to easily understand and reproduce the potential reported bugs, by clearly describing the Observed Behavior (OB), the Steps to Reproduce (S2R), and the Expected Behavior (EB). Unfortunately, while considered extremely useful, reporters often miss such pieces of information in bug reports and, to date, there is no effective way to automatically check and enforce their presence. We manually analyzed nearly 3k bug reports to understand to what extent OB, EB, and S2R are reported in bug reports and what discourse patterns reporters use to describe such information. We found that (i) while most reports contain OB (i.e., 93.5\%), only 35.2\% and 51.4\% explicitly describe EB and S2R, respectively; and (ii) reporters recurrently use 154 discourse patterns to describe such content. Based on these findings, we designed and evaluated an automated approach to detect the absence (or presence) of EB and S2R in bug descriptions. With its best setting, our approach is able to detect missing EB (S2R) with 85.9\% (69.2\%) average precision and 93.2\% (83\%) average recall. Our approach intends to improve bug descriptions quality by alerting reporters about missing EB and S2R at reporting time.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Chaparro, Oscar and Lu, Jing and Zampetti, Fiorella and Moreno, Laura and Di Penta, Massimiliano and Marcus, Andrian and Bavota, Gabriele and Ng, Vincent},
	year = {2017},
	pages = {396--407},
	annote = {Browse time: 2min 13sec, Trash; Paper premise is interesting, automating a method for determining when bug reporting information is missing, but not in line with my current area of research interests.},
	file = {Chaparro et al. - 2017 - Detecting missing information in bug descriptions.pdf:C\:\\Users\\Charles\\Zotero\\storage\\KEUJS74I\\Chaparro et al. - 2017 - Detecting missing information in bug descriptions.pdf:application/pdf}
}

@article{moreau_templating_2018,
	title = {A {Templating} {System} to {Generate} {Provenance}},
	volume = {44},
	issn = {0098-5589},
	doi = {10.1109/TSE.2017.2659745},
	abstract = {PROV-TEMPLATEIS a declarative approach that enables designers and programmers to design and generate provenance compatible with the PROV standard of the World Wide Web Consortium. Designers specify the topology of the provenance to be generated by composing templates, which are provenance graphs containing variables, acting as placeholders for values. Programmers write programs that log values and package them up in sets of bindings, a data structure associating variables and values. An expansion algorithm generates instantiated provenance from templates and sets of bindings in any of the serialisation formats supported by PROV. A quantitative evaluation shows that sets of bindings have a size that is typically 40 percent of that of expanded provenance templates and that the expansion algorithm is suitably tractable, operating in fractions of milliseconds for the type of templates surveyed in the article. Furthermore, the approach shows four significant software engineering benefits: separation of responsibilities, provenance maintenance, potential runtime checks and static analysis, and provenance consumption. The article gathers quantitative data and qualitative benefits descriptions from four different applications making use of PROV-TEMPLATE. The system is implemented and released in the open-source library ProvToolbox for provenance processing.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Moreau, L. and Batlajery, B. V. and Huynh, T. D. and Michaelides, D. and Packer, H.},
	month = feb,
	year = {2018},
	keywords = {Automobiles, data structure, data structures, Electronic publishing, expanded provenance templates, expansion algorithm, graph theory, Instruments, Internet, Libraries, Maintenance engineering, open-source library ProvToolbox, prov, PROV standard, PROV-TEMPLATE, Provenance, provenance consumption, provenance generation, provenance graphs, provenance maintenance, provenance processing, quantitative data, Runtime, software engineering, software maintenance, Standards, template, templating system, World Wide Web Consortium},
	pages = {103--121},
	annote = {Browse time: 1min 59sec, Trash; Paper details a system to generate provenance to the standard of the WWW consortium, this is not in my area of current research interest.},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Charles\\Zotero\\storage\\CV2EBBLQ\\7909036.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\Charles\\Zotero\\storage\\E5LF5IKY\\Moreau et al. - 2018 - A Templating System to Generate Provenance.pdf:application/pdf}
}

@inproceedings{maggio_automated_2017,
	address = {Paderborn, Germany},
	title = {Automated control of multiple software goals using multiple actuators},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106247},
	doi = {10.1145/3106237.3106247},
	abstract = {Modern software should satisfy multiple goals simultaneously: it should provide predictable performance, be robust to failures, handle peak loads and deal seamlessly with unexpected conditions and changes in the execution environment. For this to happen, software designs should account for the possibility of runtime changes and provide formal guarantees of the software’s behavior. Control theory is one of the possible design drivers for runtime adaptation, but adopting control theoretic principles often requires additional, specialized knowledge. To overcome this limitation, automated methodologies have been proposed to extract the necessary information from experimental data and design a control system for runtime adaptation. These proposals, however, only process one goal at a time, creating a chain of controllers. In this paper, we propose and evaluate the first automated strategy that takes into account multiple goals without separating them into multiple control strategies. Avoiding the separation allows us to tackle a larger class of problems and provide stronger guarantees. We test our methodology’s generality with three case studies that demonstrate its broad applicability in meeting performance, reliability, quality, security, and energy goals despite environmental or requirements changes.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Maggio, Martina and Papadopoulos, Alessandro Vittorio and Filieri, Antonio and Hoffmann, Henry},
	year = {2017},
	pages = {373--384},
	annote = {Browse time: 1 min 55sec, Trash; Paper details an automated methodology to design a single control system that accounts for multiple goals at runtime adaptation, this is outside of my current area of research interest but may be interesting later.},
	file = {Maggio et al. - 2017 - Automated control of multiple software goals using.pdf:C\:\\Users\\Charles\\Zotero\\storage\\68TGNK36\\Maggio et al. - 2017 - Automated control of multiple software goals using.pdf:application/pdf}
}

@inproceedings{bagherzadeh_model-level_2017,
	address = {Paderborn, Germany},
	title = {Model-level, platform-independent debugging in the context of the model-driven development of real-time systems},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106278},
	doi = {10.1145/3106237.3106278},
	abstract = {Providing proper support for debugging models at model-level is one of the main barriers to a broader adoption of Model Driven Development (MDD). In this paper, we focus on the use of MDD for the development of real-time embedded systems (RTE). We introduce a new platform-independent approach to implement model-level debuggers. We describe how to realize support for model-level debugging entirely in terms of the modeling language and show how to implement this support in terms of a model-to-model transformation. Key advantages of the approach over existing work are that (1) it does not require a program debugger for the code generated from the model, and that (2) any changes to, e.g., the code generator, the target language, or the hardware platform leave the debugger completely unaffected. We also describe an implementation of the approach in the context of Papyrus-RT, an open source MDD tool based on the modeling language UML-RT. We summarize the results of the use of our model-based debugger on several use cases to determine its overhead in terms of size and performance. Despite being a prototype, the performance overhead is in the order of microseconds, while the size overhead is comparable with that of GDB, the GNU Debugger.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Bagherzadeh, Mojtaba and Hili, Nicolas and Dingel, Juergen},
	year = {2017},
	pages = {419--430},
	annote = {Browse time: 2min 39sec, Trash; Paper details an implementation for model driven development debugging that is independent of the language and hardware, not interesting to my current research but may be useful in future research.},
	file = {Bagherzadeh et al. - 2017 - Model-level, platform-independent debugging in the.pdf:C\:\\Users\\Charles\\Zotero\\storage\\UCL2HBQ2\\Bagherzadeh et al. - 2017 - Model-level, platform-independent debugging in the.pdf:application/pdf}
}

@inproceedings{sorensen_cooperative_2017,
	address = {Paderborn, Germany},
	title = {Cooperative kernels: {GPU} multitasking for blocking algorithms},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Cooperative kernels},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106265},
	doi = {10.1145/3106237.3106265},
	abstract = {There is growing interest in accelerating irregular data-parallel algorithms on GPUs. These algorithms are typically blocking, so they require fair scheduling. But GPU programming models (e.g. OpenCL) do not mandate fair scheduling, and GPU schedulers are unfair in practice. Current approaches avoid this issue by exploiting scheduling quirks of today’s GPUs in a manner that does not allow the GPU to be shared with other workloads (such as graphics rendering tasks). We propose cooperative kernels, an extension to the traditional GPU programming model geared towards writing blocking algorithms. Workgroups of a cooperative kernel are fairly scheduled, and multitasking is supported via a small set of language extensions through which the kernel and scheduler cooperate. We describe a prototype implementation of a cooperative kernel framework implemented in OpenCL 2.0 and evaluate our approach by porting a set of blocking GPU applications to cooperative kernels and examining their performance under multitasking.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Sorensen, Tyler and Evrard, Hugues and Donaldson, Alastair F.},
	year = {2017},
	pages = {431--441},
	annote = {Browse Time: 2 min 18 sec, Trash; Paper deals using cooperative kernels to mandate fair scheduling in the GPU blocking algorithms for parallelism, not interesting to current research area but may be for future projects.},
	file = {Sorensen et al. - 2017 - Cooperative kernels GPU multitasking for blocking.pdf:C\:\\Users\\Charles\\Zotero\\storage\\KFTCUT83\\Sorensen et al. - 2017 - Cooperative kernels GPU multitasking for blocking.pdf:application/pdf}
}

@inproceedings{dietsch_craig_2017,
	address = {Paderborn, Germany},
	title = {Craig vs. {Newton} in software model checking},
	isbn = {978-1-4503-5105-8},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106307},
	doi = {10.1145/3106237.3106307},
	abstract = {Ever since the seminal work on SLAM and BLAST, software model checking with counterexample-guided abstraction refinement (CEGAR) has been an active topic of research. The crucial procedure here is to analyze a sequence of program statements (the counterexample) to find building blocks for the overall proof of the program. We can distinguish two approaches (which we name Craig and Newton) to implement the procedure. The historically first approach, Newton (named after the tool Newton from the SLAM toolkit), is based on symbolic execution. The second approach, Craig, is based on Craig interpolation. It was widely believed that Craig is substantially more effective than Newton. In fact, 12 out of the 15 CEGAR-based tools in SV-COMP are based on Craig. Advances in software model checkers based on Craig, however, can go only lockstep with advances in SMT solvers with Craig interpolation. It may be time to revisit Newton and ask whether Newton can be as effective as Craig. We have implemented a total of 11 variants of Craig and Newton in two different state-of-the-art software model checking tools and present the outcome of our experimental comparison.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Dietsch, Daniel and Heizmann, Matthias and Musa, Betim and Nutz, Alexander and Podelski, Andreas},
	year = {2017},
	pages = {487--497},
	annote = {Browse time: 2min 20sec, Trash; Paper is comparing the effectiveness of two tools for testing software models, this is not in my area of interest.},
	file = {Dietsch et al. - 2017 - Craig vs. Newton in software model checking.pdf:C\:\\Users\\Charles\\Zotero\\storage\\U5LZ5KHJ\\Dietsch et al. - 2017 - Craig vs. Newton in software model checking.pdf:application/pdf}
}

@inproceedings{cedrim_understanding_2017,
	address = {Paderborn, Germany},
	title = {Understanding the impact of refactoring on smells: a longitudinal study of 23 software projects},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Understanding the impact of refactoring on smells},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106259},
	doi = {10.1145/3106237.3106259},
	abstract = {Code smells in a program represent indications of structural quality problems, which can be addressed by software refactoring. However, refactoring intends to achieve different goals in practice, and its application may not reduce smelly structures. Developers may neglect or end up creating new code smells through refactoring. Unfortunately, little has been reported about the beneficial and harmful effects of refactoring on code smells. This paper reports a longitudinal study intended to address this gap. We analyze how often commonly-used refactoring types affect the density of 13 types of code smells along the version histories of 23 projects. Our findings are based on the analysis of 16,566 refactorings distributed in 10 different types. Even though 79.4\% of the refactorings touched smelly elements, 57\% did not reduce their occurrences. Surprisingly, only 9.7\% of refactorings removed smells, while 33.3\% induced the introduction of new ones. More than 95\% of such refactoringinduced smells were not removed in successive commits, which suggest refactorings tend to more frequently introduce long-living smells instead of eliminating existing ones. We also characterized and quantified typical refactoring-smell patterns, and observed that harmful patterns are frequent, including: (i) approximately 30\% of the Move Method and Pull Up Method refactorings induced the emergence of God Class, and (ii) the Extract Superclass refactoring creates the smell Speculative Generality in 68\% of the cases.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Cedrim, Diego and Garcia, Alessandro and Mongiovi, Melina and Gheyi, Rohit and Sousa, Leonardo and de Mello, Rafael and Fonseca, Baldoino and Ribeiro, Márcio and Chávez, Alexander},
	year = {2017},
	pages = {465--475},
	annote = {Browse Time: 1 min 58 sec, Scan; The idea that refactoring removes code smells and bad pathing is prevalent and I find it interesting that this paper seemingly dispels this notion with its results, this is in my current area of interest.},
	file = {Cedrim et al. - 2017 - Understanding the impact of refactoring on smells.pdf:C\:\\Users\\Charles\\Zotero\\storage\\KFQVA3DE\\Cedrim et al. - 2017 - Understanding the impact of refactoring on smells.pdf:application/pdf}
}

@inproceedings{galhotra_fairness_2017,
	address = {Paderborn, Germany},
	title = {Fairness testing: testing software for discrimination},
	isbn = {978-1-4503-5105-8},
	shorttitle = {Fairness testing},
	url = {http://dl.acm.org/citation.cfm?doid=3106237.3106277},
	doi = {10.1145/3106237.3106277},
	abstract = {This paper defines software fairness and discrimination and develops a testing-based method for measuring if and how much software discriminates, focusing on causality in discriminatory behavior. Evidence of software discrimination has been found in modern software systems that recommend criminal sentences, grant access to financial products, and determine who is allowed to participate in promotions. Our approach, Themis, generates efficient test suites to measure discrimination. Given a schema describing valid system inputs, Themis generates discrimination tests automatically and does not require an oracle. We evaluate Themis on 20 software systems, 12 of which come from prior work with explicit focus on avoiding discrimination. We find that (1) Themis is effective at discovering software discrimination, (2) state-of-the-art techniques for removing discrimination from algorithms fail in many situations, at times discriminating against as much as 98\% of an input subdomain, (3) Themis optimizations are effective at producing efficient test suites for measuring discrimination, and (4) Themis is more efficient on systems that exhibit more discrimination. We thus demonstrate that fairness testing is a critical aspect of the software development cycle in domains with possible discrimination and provide initial tools for measuring software discrimination.},
	language = {en},
	urldate = {2018-09-08},
	booktitle = {Proceedings of the 2017 11th {Joint} {Meeting} on {Foundations} of {Software} {Engineering}  - {ESEC}/{FSE} 2017},
	publisher = {ACM Press},
	author = {Galhotra, Sainyam and Brun, Yuriy and Meliou, Alexandra},
	year = {2017},
	pages = {498--510},
	annote = {Browse time: 2mins 4sec, Trash; I find it rather interesting that software can be discriminatory in nature, while it makes sense since it is being written by humans with their own bias, I never would've considered it. This paper is outside of my current area of research but I would like to revisit it for future research options.},
	file = {Galhotra et al. - 2017 - Fairness testing testing software for discriminat.pdf:C\:\\Users\\Charles\\Zotero\\storage\\MNINIM49\\Galhotra et al. - 2017 - Fairness testing testing software for discriminat.pdf:application/pdf}
}